# Default values for budibase.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

imagePullSecrets: []
nameOverride: ""
# fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext:
  {}
  # fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 10000

ingress:
  enabled: true
  className: ""
  annotations:
    # nginx.ingress.kubernetes.io/client-max-body-size: 150M
    # nginx.ingress.kubernetes.io/proxy-body-size: 50m
  hosts:
    - host: # change if using custom domain
      paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: proxy-service
              port:
                number: 10000

awsAlbIngress:
  enabled: false
  certificateArn: ""

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

globals:
  appVersion: "" # Use as an override to .Chart.AppVersion
  budibaseEnv: PRODUCTION
  tenantFeatureFlags: "*:LICENSING,*:USER_GROUPS,*:ONBOARDING_TOUR"
  enableAnalytics: "1"
  posthogToken: "phc_bIjZL7oh2GEUd2vqvTBH8WvrX0fWTFQMs6H5KQxiUxU"
  selfHosted: "1" # set to 0 for budibase cloud environment, set to 1 for self-hosted setup
  multiTenancy: "0" # set to 0 to disable multiple orgs, set to 1 to enable multiple orgs
  offlineMode: "0" # set to 1 to enable offline mode
  accountPortalUrl: ""
  accountPortalApiKey: ""
  cookieDomain: ""
  platformUrl: ""
  httpMigrations: "0"
  google:
    clientId: ""
    secret: ""
  automationMaxIterations: "200"

  createSecrets: true # creates an internal API key, JWT secrets and redis password for you

  # if createSecrets is set to false, you can hard-code your secrets here
  apiEncryptionKey: ""
  internalApiKey: ""
  jwtSecret: ""
  cdnUrl: ""
  # fallback values used during live rotation
  internalApiKeyFallback: ""
  jwtSecretFallback: ""

  smtp:
    enabled: false

#  globalAgentHttpProxy:
#  globalAgentHttpsProxy:
#  globalAgentNoProxy:

services:
  dns: cluster.local
  # tlsRejectUnauthorized: 0

  proxy:
    port: 10000
    replicaCount: 1
    upstreams:
      apps: "http://app-service.{{ .Release.Namespace }}.svc.{{ .Values.services.dns }}:{{ .Values.services.apps.port }}"
      worker: "http://worker-service.{{ .Release.Namespace }}.svc.{{ .Values.services.dns }}:{{ .Values.services.worker.port }}"
      minio: "http://minio-service.{{ .Release.Namespace }}.svc.{{ .Values.services.dns }}:{{ .Values.services.objectStore.port }}"
      couchdb: "http://{{ .Release.Name }}-svc-couchdb:{{ .Values.services.couchdb.port }}"
    resources: {}
    startupProbe:
      httpGet:
        path: /health
        port: 10000
        scheme: HTTP
      failureThreshold: 30
      periodSeconds: 3
    readinessProbe:
      httpGet:
        path: /health
        port: 10000
        scheme: HTTP
      periodSeconds: 3
      failureThreshold: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 10000
        scheme: HTTP
      failureThreshold: 3
      periodSeconds: 5

  apps:
    port: 4002
    replicaCount: 1
    logLevel: info
    httpLogging: 1
    resources: {}
    startupProbe:
      httpGet:
        path: /health
        port: 4002
        scheme: HTTP
      failureThreshold: 30
      periodSeconds: 3
    readinessProbe:
      httpGet:
        path: /health
        port: 4002
        scheme: HTTP
      periodSeconds: 3
      failureThreshold: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 4002
        scheme: HTTP
      failureThreshold: 3
      periodSeconds: 5
  #    nodeDebug: "" # set the value of NODE_DEBUG
  #    annotations:
  #      co.elastic.logs/multiline.type: pattern
  #      co.elastic.logs/multiline.pattern: '^[[:space:]]'
  #      co.elastic.logs/multiline.negate: false
  #      co.elastic.logs/multiline.match: after
  worker:
    port: 4003
    replicaCount: 1
    logLevel: info
    httpLogging: 1
    resources: {}
    startupProbe:
      httpGet:
        path: /health
        port: 4003
        scheme: HTTP
      failureThreshold: 30
      periodSeconds: 3
    readinessProbe:
      httpGet:
        path: /health
        port: 4003
        scheme: HTTP
      periodSeconds: 3
      failureThreshold: 1
    livenessProbe:
      httpGet:
        path: /health
        port: 4003
        scheme: HTTP
      failureThreshold: 3
      periodSeconds: 5
  #    annotations:
  #      co.elastic.logs/multiline.type: pattern
  #      co.elastic.logs/multiline.pattern: '^[[:space:]]'
  #      co.elastic.logs/multiline.negate: false
  #      co.elastic.logs/multiline.match: after

  couchdb:
    enabled: true
    # url: "" # only change if pointing to existing couch server
    # user: "" # only change if pointing to existing couch server
    # password: "" # only change if pointing to existing couch server
    port: 5984
    backup:
      enabled: false
      # target couchDB instance to back up to
      target: ""
      # backup interval in seconds
      interval: ""
      resources: {}

  redis:
    enabled: true # disable if using external redis
    port: 6379
    replicaCount: 1
    url: "" # only change if pointing to existing redis cluster and enabled: false
    password: "budibase" # recommended to override if using built-in redis
    storage: 100Mi
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.
    storageClass: ""
    resources: {}

  objectStore:
    # Set to false if using another object store such as S3
    minio: true
    browser: true
    port: 9000
    replicaCount: 1
    accessKey: "" # AWS_ACCESS_KEY if using S3 or existing minio access key
    secretKey: "" # AWS_SECRET_ACCESS_KEY if using S3 or existing minio secret
    region: "" # AWS_REGION if using S3 or existing minio secret
    url: "http://minio-service:9000" # only change if pointing to existing minio cluster or S3 and minio: false
    storage: 100Mi
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.
    storageClass: ""
    resources: {}
    cloudfront:
      # Set the url of a distribution to enable cloudfront
      cdn: ""
      # ID of public key stored in cloudfront
      publicKeyId: ""
      # Base64 encoded private key for the above public key
      privateKey64: ""

# Override values in couchDB subchart
couchdb:
  # -- the initial number of nodes in the CouchDB cluster.
  clusterSize: 3

  # -- If allowAdminParty is enabled the cluster will start up without any database
  # administrator account; i.e., all users will be granted administrative
  # access. Otherwise, the system will look for a Secret called
  # <ReleaseName>-couchdb containing `adminUsername`, `adminPassword` and
  # `cookieAuthSecret` keys. See the `createAdminSecret` flag.
  # ref: https://kubernetes.io/docs/concepts/configuration/secret/
  allowAdminParty: false

  # -- If createAdminSecret is enabled a Secret called <ReleaseName>-couchdb will
  # be created containing auto-generated credentials. Users who prefer to set
  # these values themselves have a couple of options:
  #
  # 1) The `adminUsername`, `adminPassword`, `adminHash`, and `cookieAuthSecret`
  #    can be defined directly in the chart's values. Note that all of a chart's
  #    values are currently stored in plaintext in a ConfigMap in the tiller
  #    namespace.
  #
  # 2) This flag can be disabled and a Secret with the required keys can be
  #    created ahead of time.
  createAdminSecret: true

  adminUsername: admin
  # adminPassword: this_is_not_secure
  # adminHash: -pbkdf2-this_is_not_necessarily_secure_either
  # cookieAuthSecret: neither_is_this

  ## When enabled, will deploy a networkpolicy that allows CouchDB pods to
  ## communicate with each other for clustering and ingress on port 5984
  networkPolicy:
    enabled: true

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  # Use a service account
  serviceAccount:
    enabled: true
    create: true
  # name:
  # imagePullSecrets:
  # - name: myimagepullsecret

  # -- The storage volume used by each Pod in the StatefulSet. If a
  # persistentVolume is not enabled, the Pods will use `emptyDir` ephemeral
  # local storage. Setting the storageClass attribute to "-" disables dynamic
  # provisioning of Persistent Volumes; leaving it unset will invoke the default
  # provisioner.
  persistentVolume:
    enabled: false
    # NOTE: the number of existing claims must match the cluster size
    existingClaims: []
    annotations: {}
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    # storageClass: "-"

  ## The CouchDB image
  image:
    repository: budibase/couchdb
    tag: v3.2.1
    pullPolicy: IfNotPresent

  # -- Flip this to flag to include the Search container in each Pod
  enableSearch: false

  initImage:
    repository: busybox
    tag: latest
    pullPolicy: Always

  ## CouchDB is happy to spin up cluster nodes in parallel, but if you encounter
  ## problems you can try setting podManagementPolicy to the StatefulSet default
  ## `OrderedReady`
  podManagementPolicy: Parallel

  ## To better tolerate Node failures, we can prevent Kubernetes scheduler from
  ## assigning more than one Pod of CouchDB StatefulSet per Node using podAntiAffinity.
  affinity:
    {}
    # podAntiAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     - labelSelector:
    #         matchExpressions:
    #           - key: "app"
    #             operator: In
    #             values:
    #             - couchdb
    #       topologyKey: "kubernetes.io/hostname"

  ## To control how Pods are spread across your cluster among failure-domains such as regions,
  ## zones, nodes, and other user-defined topology domains use topologySpreadConstraints.
  topologySpreadConstraints:
    {}
    # topologySpreadConstraints:
    #   - maxSkew: 1
    #     topologyKey: "topology.kubernetes.io/zone"
    #     whenUnsatisfiable: ScheduleAnyway
    #     labelSelector:
    #       matchLabels:
    #         app: couchdb

  ## Optional pod labels
  labels: {}

  ## Optional pod annotations
  annotations: {}

  ## Optional tolerations
  tolerations: []

  ## A StatefulSet requires a headless Service to establish the stable network
  ## identities of the Pods, and that Service is created automatically by this
  ## chart without any additional configuration. The Service block below refers
  ## to a second Service that governs how clients connect to the CouchDB cluster.
  service:
    annotations: {}
    enabled: true
    type: ClusterIP
    externalPort: 5984
    targetPort: 5984
    labels: {}

  ## An Ingress resource can provide name-based virtual hosting and TLS
  ## termination among other things for CouchDB deployments which are accessed
  ## from outside the Kubernetes cluster.
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
  ingress:
    enabled: false
    # className: nginx
    hosts:
      - chart-example.local
    path: /
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    tls:
      # Secrets must be manually created in the namespace.
      # - secretName: chart-example-tls
      #   hosts:
      #     - chart-example.local

  ## Optional resource requests and limits for the CouchDB container
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    {}
    # requests:
    #  cpu: 100m
    #  memory: 128Mi
    # limits:
    #  cpu: 56
    #  memory: 256Gi

  ## Optional resource requests and limits for the CouchDB init container
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  initResources:
    {}
    # requests:
    #  cpu: 100m
    #  memory: 128Mi
    # limits:
    #  cpu: 500m
    #  memory: 128Mi

  # -- erlangFlags is a map that is passed to the Erlang VM as flags using the
  # ERL_FLAGS env. The `name` flag is required to establish connectivity
  # between cluster nodes.
  # ref: http://erlang.org/doc/man/erl.html#init_flags
  erlangFlags:
    name: couchdb
    # Older versions of the official CouchDB image (anything prior to 3.2.1)
    # do not act on the COUCHDB_ERLANG_COOKIE environment variable, so if you
    # want to cluster these deployments it's necessary to pass in a cookie here
    # setcookie: make-something-up

  # -- couchdbConfig will override default CouchDB configuration settings.
  # The contents of this map are reformatted into a .ini file laid down
  # by a ConfigMap object.
  # ref: http://docs.couchdb.org/en/latest/config/index.html
  couchdbConfig:
    couchdb:
      uuid: decafbaddecafbaddecafbaddecafbad # Unique identifier for this CouchDB server instance
    # cluster:
    #   q: 8 # Create 8 shards for each database
    chttpd:
      bind_address: any
      # chttpd.require_valid_user disables all the anonymous requests to the port
      # 5984 when is set to true.
      require_valid_user: false
    # required to use Fauxton if chttpd.require_valid_user is set to true
    # httpd:
    #   WWW-Authenticate: "Basic realm=\"administrator\""

  # Kubernetes local cluster domain.
  # This is used to generate FQDNs for peers when joining the CouchDB cluster.
  dns:
    clusterDomainSuffix: cluster.local

  ## Configure liveness and readiness probe values
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  livenessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 0
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  readinessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 0
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1

  # Control an optional pod disruption budget
  podDisruptionBudget:
    # toggle creation of pod disruption budget, disabled by default
    enabled: false
    # minAvailable: 1
    maxUnavailable: 1

  # CouchDB 3.2.0 adds in a metrics endpoint on the path `/_node/_local/_prometheus`.
  # Optionally, a standalone, unauthenticated port can be exposed for these metrics.
  prometheusPort:
    enabled: false
    bind_address: "0.0.0.0"
    port: 17986

  # Configure arbitrary sidecar containers for CouchDB pods created by the
  # StatefulSet
  sidecars:
    {}
    # - name: foo
    #   image: "busybox"
    #   imagePullPolicy: IfNotPresent
    #   resources:
    #     requests:
    #       cpu: "0.1"
    #       memory: 10Mi
    #   command: ['echo "foo";']
    #   volumeMounts:
    #     - name: database-storage
    #       mountPath: /opt/couchdb/data/

  # Placement manager to annotate each document in the nodes DB with "zone" attribute
  # recording the zone where node has been scheduled
  # Ref: https://docs.couchdb.org/en/stable/cluster/sharding.html#specifying-database-placement
  placementConfig:
    enabled: false
    image:
      repository: caligrafix/couchdb-autoscaler-placement-manager
      tag: 0.1.0
